{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from document_extractor import FinancialDocumentProcessor\n",
    "from typing import TypedDict, Optional, Dict, Any\n",
    "\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, OpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from docling.datamodel.document import ConversionResult\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultEvaluator():\n",
    "    def __init__(self, \n",
    "        model_name: str = \"gpt-4o-mini\",\n",
    "        openai_api_key: Optional[str] = None,\n",
    "        temperature: float = 0.0,\n",
    "        max_tokens: int = 512):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "        self.llm = OpenAI(\n",
    "            api_key=self.api_key,\n",
    "            model=self.model_name,\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens,\n",
    "        )\n",
    "\n",
    "    def evaluate_bs_item(self, bs_item):\n",
    "        \n",
    "        try:\n",
    "            assert bs_item[\"total_assets\"] >= bs_item[\"total_liabilities\"], \"Liabilities exceed assets\"\n",
    "            assert bs_item[\"investment_properties\"] <= bs_item[\"total_assets\"], \"Investment properties exceed total assets\"\n",
    "            assert bs_item[\"total_debt\"] <= bs_item[\"total_liabilities\"], \"Debt exceeds liabilities\"\n",
    "            assert bs_item[\"nta_per_unit\"] > 0, \"NTA per unit is non-positive\"\n",
    "            return {\"verdict\": \"Correct\", \"justification\": \"All checks passed\"}\n",
    " \n",
    "\n",
    "        except AssertionError as e:\n",
    "            return {\"verdict\": \"Incorrect\", \"justification\": str(e)}\n",
    "\n",
    "    \n",
    "    def evaluate_is_item(self, is_item):\n",
    "        \n",
    "        try:\n",
    "            assert is_item[\"total_revenue\"] >= 0, \"Total assets cannot be negative\"\n",
    "            return {\"verdict\": \"Correct\", \"justification\": \"All checks passed\"}\n",
    "\n",
    "        except AssertionError as e:\n",
    "            return {\"verdict\": \"Incorrect\", \"justification\": str(e)}\n",
    "\n",
    "    \n",
    "    def evaluate(self, context, input:dict):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a rigorous auditor evaluating the output of a financial information extraction system.\n",
    "        Return your verdict in the following format:\n",
    "        ```json\n",
    "        {\n",
    "        \"verdict\": \"Correct\" or \"Incorrect\",\n",
    "        \"justification\": \"Short explanation of why the output was deemed correct or not.\"\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        template = PromptTemplate.from_template(\n",
    "            \"{system_prompt}\\n\"\n",
    "            \"Context information is below.\\n---------------------\\n{context}\\n---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge, answer the query.\\n\"\n",
    "            \"Query: Evaluate whether the extracted information in the json {input} is consistent with what is available in the context. Note that unit conversion may have occurred\",\n",
    "        )\n",
    "\n",
    "        chain = template | self.llm\n",
    "        input_dict = {'input': str(input),\"system_prompt\": system_prompt,\"context\": context}\n",
    "        result = chain.invoke(input_dict)\n",
    "        print(\"llm_output\",result)\n",
    "        # prompt = f\"{system_prompt} . The BS item is {str(bs_item)} and the IS item is {str(is_item)}. Evaluate the extracted information and provide your verdict.\"\n",
    "\n",
    "        return result\n",
    "        \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractorState(TypedDict):\n",
    "    file_name: str\n",
    "    # conversion_result = Optional[ConversionResult]\n",
    "    bs_result: Optional[dict]\n",
    "    is_result: Optional[dict]\n",
    "    feedback: Optional[str]\n",
    "    retries: int\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "fdp = FinancialDocumentProcessor(data_dir=\"datasets/co_presentations\")\n",
    "evaluator = ResultEvaluator()\n",
    "\n",
    "# Actor Node\n",
    "def actor_node(state: ExtractorState) -> ExtractorState:\n",
    "    print(f\"Running actor on {state['file_name']}\")\n",
    "\n",
    "    fdp.ingest_document(state[\"file_name\"])\n",
    "    fdp.extract_tables()\n",
    "    extracted_info = fdp.extract_information()\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        # \"conversion_result\":fdp.result,\n",
    "        \"bs_result\": extracted_info[0],\n",
    "        \"is_result\": extracted_info[1],\n",
    "        \"retries\": state[\"retries\"] + 1,\n",
    "    }\n",
    "\n",
    "# Critic Node\n",
    "def critic_node(state: ExtractorState) -> ExtractorState:\n",
    "    print(\"Critic evaluating...\")\n",
    "    \n",
    "    bs_result = state[\"bs_result\"]\n",
    "    is_result = state[\"is_result\"]\n",
    "    # verdict = evaluator.evaluate_bs_item(state[\"bs_result\"])[\"verdict\"] and evaluator.evaluate_is_item(state[\"is_result\"])[\"verdict\"]\n",
    "    verdict_bs = evaluator.evaluate(context = fdp.result.document.export_to_markdown(), input = bs_result)[\"verdict\"]\n",
    "    print(\"verdict_bs\", verdict_bs)\n",
    "    verdict_is = evaluator.evaluate(context = fdp.result.document.export_to_markdown(), input = is_result)[\"verdict\"]\n",
    "    print(\"verdict_is\", verdict_is)\n",
    "    \n",
    "    verdict = \"Correct\" if verdict_bs == \"Correct\" and verdict_is == \"Correct\" else \"Incorrect\"\n",
    "\n",
    "    if verdict == \"Correct\":\n",
    "        print(\"Critic: All checks passed.\")\n",
    "    else:\n",
    "        print(\"Critic: Some checks failed.\")\n",
    "    return {**state, \"feedback\": verdict}\n",
    "\n",
    "\n",
    "\n",
    "actor_runnable = RunnableLambda(actor_node)\n",
    "critic_runnable = RunnableLambda(critic_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "graph = StateGraph(ExtractorState)\n",
    "\n",
    "graph.add_node(\"actor\", actor_runnable)\n",
    "graph.add_node(\"critic\", critic_runnable)\n",
    "\n",
    "# Sequence: start at actor â†’ critic\n",
    "graph.set_entry_point(\"actor\")\n",
    "graph.add_edge(\"actor\", \"critic\")\n",
    "\n",
    "# Conditional: loop back if incorrect\n",
    "graph.add_conditional_edges(\n",
    "    \"critic\",\n",
    "    lambda state: \"actor\" if state[\"feedback\"] != \"Correct\" and state[\"retries\"] < 3 else \"__end__\"\n",
    ")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running actor on CLW_HY25_IP.pdf\n",
      "Processing file: CLW_HY25_IP.pdf\n"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "    \"file_name\": \"CLW_HY25_IP.pdf\",\n",
    "    \"retries\": 0,\n",
    "    \"bs_result\": None,\n",
    "    \"is_result\": None,\n",
    "    \"feedback\": None,\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "print(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
